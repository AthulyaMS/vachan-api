version: "3.9"

x-clickhouse-defaults: &clickhouse-defaults
  restart: on-failure
  # addding non LTS version due to this fix https://github.com/ClickHouse/ClickHouse/commit/32caf8716352f45c1b617274c7508c86b7d1afab
  image: clickhouse/clickhouse-server:24.1.2-alpine
  tty: true
  depends_on:
    - zookeeper-1
    # - zookeeper-2
    # - zookeeper-3
  logging:
    options:
      max-size: 50m
      max-file: "3"
  healthcheck:
    # "clickhouse", "client", "-u ${CLICKHOUSE_USER}", "--password ${CLICKHOUSE_PASSWORD}", "-q 'SELECT 1'"
    test:
      [
        "CMD",
        "wget",
        "--spider",
        "-q",
        "localhost:8123/ping"
      ]
    interval: 30s
    timeout: 5s
    retries: 3
  ulimits:
    nproc: 65535
    nofile:
      soft: 262144
      hard: 262144

x-db-depend: &db-depend
  depends_on:
    clickhouse:
      condition: service_healthy
    otel-collector-migrator:
      condition: service_completed_successfully
    # clickhouse-2:
    #   condition: service_healthy
    # clickhouse-3:
    #   condition: service_healthy

services:



  vachan-cms-rest:
    image: athulyams/vachan-cms-rest:signoz3
    expose:
      - 8005
    command: opentelemetry-instrument uvicorn main:app --host 0.0.0.0 --port 8005
    restart: always
    environment:
     - VACHAN_POSTGRES_HOST=vachan-db
     - VACHAN_POSTGRES_PORT=5432
     - VACHAN_POSTGRES_USER=${VACHAN_POSTGRES_USER:-postgres}
     - VACHAN_POSTGRES_PASSWORD=${VACHAN_POSTGRES_PASSWORD:-password}
     - VACHAN_POSTGRES_DATABASE=${VACHAN_POSTGRES_DATABASE:-vachan_dev}
     - VACHAN_POSTGRES_SCHEMA=vachan_cms_rest
     - VACHAN_LOGGING_LEVEL=INFO
     - VACHAN_DOMAIN=${VACHAN_DOMAIN:-http://localhost}
     - VACHAN_CMS_DOMAIN=http://vachan-api
    #  Add OpenTelemetry environment variables
     - OTEL_EXPORTER_OTLP_ENDPOINT=http://signoz-otel-collector:4317
    volumes:
     - ./logs-vol:/app/logs
    depends_on:
     - vachan-db
     - otel-collector  # Ensure Signoz OTel Collector is up before your service
    # profiles:
    #  - local-run
    #  - deployment
    # networks:
    #  - VE-network


  vachan-db:
    image: postgres:16.0
    healthcheck:
     test: [ "CMD", "pg_isready", "-q", "-d", "postgres", "-U", "postgres" ]
     timeout: 45s
     interval: 10s
     retries: 10
    restart: always
    environment:
     - POSTGRES_USER=${VACHAN_POSTGRES_USER:-postgres}
     - POSTGRES_PASSWORD=${VACHAN_POSTGRES_PASSWORD:-password}
     - POSTGRES_DB=${VACHAN_POSTGRES_DATABASE:-vachan_dev}
     - POSTGRES_HOST_AUTH_METHOD=trust
    logging:
     options:
      max-size: 10m
      max-file: "3"
    expose:
     - 5432
    ports:
     # HOST:CONTAINER
     - "7777:5432"
    # networks:
    #  - VE-network
    volumes:
     # - ${VACHAN_POSTGRES_DATA_DIR:-./pgdata}:/var/lib/postgresql/data
     - ./vachan-db-vol:/var/lib/postgresql/data
     - .../db/csvs:/csvs
     - .../db/seed_DB.sql:/docker-entrypoint-initdb.d/seed_DB.sql
     - ./vachan-db-backup:/var/backups:rw
    labels:
      ofelia.enabled: "true"
      ofelia.job-exec.vachan-backup.schedule: "0 0 0 * * *" #"Takes a single dump within the db container every day at 00:00 am"
      ofelia.job-exec.vachan-backup.command: "pg_dump --dbname=postgresql://$VACHAN_POSTGRES_USER:$VACHAN_POSTGRES_PASSWORD@localhost:5432/$VACHAN_POSTGRES_DATABASE  --file=/var/backups/vachan_db_backup_latest.sql"


  zookeeper-1:
    image: bitnami/zookeeper:3.7.1
    container_name: signoz-zookeeper-1
    hostname: zookeeper-1
    user: root
    ports:
      - "2181:2181"
      - "2888:2888"
      - "3888:3888"
    volumes:
      - ./data/zookeeper-1:/bitnami/zookeeper
    environment:
      - ZOO_SERVER_ID=1
      # - ZOO_SERVERS=0.0.0.0:2888:3888,zookeeper-2:2888:3888,zookeeper-3:2888:3888
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_AUTOPURGE_INTERVAL=1

  # zookeeper-2:
  #   image: bitnami/zookeeper:3.7.0
  #   container_name: signoz-zookeeper-2
  #   hostname: zookeeper-2
  #   user: root
  #   ports:
  #     - "2182:2181"
  #     - "2889:2888"
  #     - "3889:3888"
  #   volumes:
  #     - ./data/zookeeper-2:/bitnami/zookeeper
  #   environment:
  #     - ZOO_SERVER_ID=2
  #     - ZOO_SERVERS=zookeeper-1:2888:3888,0.0.0.0:2888:3888,zookeeper-3:2888:3888
  #     - ALLOW_ANONYMOUS_LOGIN=yes
  #     - ZOO_AUTOPURGE_INTERVAL=1

  # zookeeper-3:
  #   image: bitnami/zookeeper:3.7.0
  #   container_name: signoz-zookeeper-3
  #   hostname: zookeeper-3
  #   user: root
  #   ports:
  #     - "2183:2181"
  #     - "2890:2888"
  #     - "3890:3888"
  #   volumes:
  #     - ./data/zookeeper-3:/bitnami/zookeeper
  #   environment:
  #     - ZOO_SERVER_ID=3
  #     - ZOO_SERVERS=zookeeper-1:2888:3888,zookeeper-2:2888:3888,0.0.0.0:2888:3888
  #     - ALLOW_ANONYMOUS_LOGIN=yes
  #     - ZOO_AUTOPURGE_INTERVAL=1

  clickhouse:
    <<: *clickhouse-defaults
    container_name: signoz-clickhouse
    hostname: clickhouse
    ports:
      - "9000:9000"
      - "8123:8123"
      - "9181:9181"
    volumes:
      - ./clickhouse-config.xml:/etc/clickhouse-server/config.xml
      - ./clickhouse-users.xml:/etc/clickhouse-server/users.xml
      - ./custom-function.xml:/etc/clickhouse-server/custom-function.xml
      - ./clickhouse-cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
      # - ./clickhouse-storage.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./data/clickhouse/:/var/lib/clickhouse/
      - ./user_scripts:/var/lib/clickhouse/user_scripts/

  # clickhouse-2:
  #   <<: *clickhouse-defaults
  #   container_name: signoz-clickhouse-2
  #   hostname: clickhouse-2
  #   ports:
  #     - "9001:9000"
  #     - "8124:8123"
  #     - "9182:9181"
  #   volumes:
  #     - ./clickhouse-config.xml:/etc/clickhouse-server/config.xml
  #     - ./clickhouse-users.xml:/etc/clickhouse-server/users.xml
  #     - ./custom-function.xml:/etc/clickhouse-server/custom-function.xml
  #     - ./clickhouse-cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
  #     # - ./clickhouse-storage.xml:/etc/clickhouse-server/config.d/storage.xml
  #     - ./data/clickhouse-2/:/var/lib/clickhouse/
  #     - ./user_scripts:/var/lib/clickhouse/user_scripts/


  # clickhouse-3:
  #   <<: *clickhouse-defaults
  #   container_name: signoz-clickhouse-3
  #   hostname: clickhouse-3
  #   ports:
  #     - "9002:9000"
  #     - "8125:8123"
  #     - "9183:9181"
  #   volumes:
  #     - ./clickhouse-config.xml:/etc/clickhouse-server/config.xml
  #     - ./clickhouse-users.xml:/etc/clickhouse-server/users.xml
  #     - ./custom-function.xml:/etc/clickhouse-server/custom-function.xml
  #     - ./clickhouse-cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
  #     # - ./clickhouse-storage.xml:/etc/clickhouse-server/config.d/storage.xml
  #     - ./data/clickhouse-3/:/var/lib/clickhouse/
  #     - ./user_scripts:/var/lib/clickhouse/user_scripts/

  alertmanager:
    image: signoz/alertmanager:${ALERTMANAGER_TAG:-0.23.5}
    container_name: signoz-alertmanager
    volumes:
      - ./data/alertmanager:/data
    depends_on:
      query-service:
        condition: service_healthy
    restart: on-failure
    command:
      - --queryService.url=http://query-service:8085
      - --storage.path=/data

  # Notes for Maintainers/Contributors who will change Line Numbers of Frontend & Query-Section. Please Update Line Numbers in `./scripts/commentLinesForSetup.sh` & `./CONTRIBUTING.md`

  query-service:
    image: signoz/query-service:${DOCKER_TAG:-0.39.0}
    container_name: signoz-query-service
    command:
      [
        "-config=/root/config/prometheus.yml",
        # "--prefer-delta=true"
      ]
    # ports:
    #   - "6060:6060"     # pprof port
    #   - "8080:8080"     # query-service port
    volumes:
      - ./prometheus.yml:/root/config/prometheus.yml
      - ../dashboards:/root/config/dashboards
      - ./data/signoz/:/var/lib/signoz/
    environment:
      - ClickHouseUrl=tcp://clickhouse:9000
      - ALERTMANAGER_API_PREFIX=http://alertmanager:9093/api/
      - SIGNOZ_LOCAL_DB_PATH=/var/lib/signoz/signoz.db
      - DASHBOARDS_PATH=/root/config/dashboards
      - STORAGE=clickhouse
      - GODEBUG=netdns=go
      - TELEMETRY_ENABLED=true
      - DEPLOYMENT_TYPE=docker-standalone-amd
    restart: on-failure
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--spider",
          "-q",
          "localhost:8080/api/v1/health"
        ]
      interval: 30s
      timeout: 5s
      retries: 3
    <<: *db-depend

  frontend:
    image: signoz/frontend:${DOCKER_TAG:-0.39.0}
    container_name: signoz-frontend
    restart: on-failure
    depends_on:
      - alertmanager
      - query-service
    ports:
      - "3301:3301"
    volumes:
      - ../common/nginx-config.conf:/etc/nginx/conf.d/default.conf

  otel-collector-migrator:
    image: signoz/signoz-schema-migrator:${OTELCOL_TAG:-0.88.12}
    container_name: otel-migrator
    command:
      - "--dsn=tcp://clickhouse:9000"
    depends_on:
      clickhouse:
        condition: service_healthy
      # clickhouse-2:
      #   condition: service_healthy
      # clickhouse-3:
      #   condition: service_healthy


  otel-collector:
    image: signoz/signoz-otel-collector:${OTELCOL_TAG:-0.88.12}
    container_name: signoz-otel-collector
    command:
      [
        "--config=/etc/otel-collector-config.yaml",
        "--manager-config=/etc/manager-config.yaml",
        "--copy-path=/var/tmp/collector-config.yaml",
        "--feature-gates=-pkg.translator.prometheus.NormalizeName"
      ]
    user: root # required for reading docker container logs
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
      - ./otel-collector-opamp-config.yaml:/etc/manager-config.yaml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    environment:
      - OTEL_RESOURCE_ATTRIBUTES=host.name=signoz-host,os.type=linux
      - DOCKER_MULTI_NODE_CLUSTER=false
      - LOW_CARDINAL_EXCEPTION_GROUPING=false
    ports:
      # - "1777:1777"     # pprof extension
      - "4317:4317" # OTLP gRPC receiver
      - "4318:4318" # OTLP HTTP receiver
      # - "8888:8888"     # OtelCollector internal metrics
      # - "8889:8889"     # signoz spanmetrics exposed by the agent
      # - "9411:9411"     # Zipkin port
      # - "13133:13133"   # health check extension
      # - "14250:14250"   # Jaeger gRPC
      # - "14268:14268"   # Jaeger thrift HTTP
      # - "55678:55678"   # OpenCensus receiver
      # - "55679:55679"   # zPages extension
    restart: on-failure
    depends_on:
      clickhouse:
        condition: service_healthy
      otel-collector-migrator:
        condition: service_completed_successfully
      query-service:
        condition: service_healthy

  logspout:
    image: "gliderlabs/logspout:v3.2.14"
    container_name: signoz-logspout
    volumes:
      - /etc/hostname:/etc/host_hostname:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: syslog+tcp://otel-collector:2255
    depends_on:
      - otel-collector
    restart: on-failure

  

  
  

  
  hotrod:
    image: jaegertracing/example-hotrod:1.30
    container_name: hotrod
    logging:
      options:
        max-size: 50m
        max-file: "3"
    command: [ "all" ]
    environment:
      - JAEGER_ENDPOINT=http://otel-collector:14268/api/traces

  load-hotrod:
    image: "signoz/locust:1.2.3"
    container_name: load-hotrod
    hostname: load-hotrod
    environment:
      ATTACKED_HOST: http://hotrod:8080
      LOCUST_MODE: standalone
      NO_PROXY: standalone
      TASK_DELAY_FROM: 5
      TASK_DELAY_TO: 30
      QUIET_MODE: "${QUIET_MODE:-false}"
      LOCUST_OPTS: "--headless -u 10 -r 1"
    volumes:
      - ../common/locust-scripts:/locust


  # Web Server
  web-server-local:
    image: nginx:latest
    ports:
     - 82:80
    restart: always
    expose:
     - 80
    depends_on:
     - vachan-cms-rest
    volumes:
     - ../nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
     - ./logs-vol:/var/log/nginx/
    # profiles:
    #  - local-run
    # networks:
    #  - VE-network

  # web-server-with-cert:
  #   image: nginx:latest
  #   ports:
  #   - 84:80
  #   - 443:443
  #   restart: always
  #   expose:
  #   - 80
  #   - 443
  #   depends_on:
  #   - vachan-cms-rest
  #   volumes:
  #   - ../nginx/prod/app.conf.template:/etc/nginx/templates/default.conf.template:ro
  #   - ../certbot/www:/var/www/certbot/:ro
  #   - ../certbot/conf/:/etc/nginx/ssl/:ro
  #   - ./logs-vol:/var/log/nginx/
  #   environment:
  #     - VACHAN_DOMAIN=${VACHAN_DOMAIN}
  #   profiles:
  #   - deployment
  #   networks:
  #   - VE-network

  # certbot:
  #   image: certbot/certbot:latest
  #   volumes:
  #   - ../certbot/www/:/var/www/certbot/:rw
  #   - ../certbot/conf/:/etc/letsencrypt/:rw
  #   profiles:
  #   - deployment
  #   networks:
  #   - VE-network
 


  
# networks:
#    VE-network:

volumes:
  vachan-db-vol:
  logs-vol:
  vachan-db-backup:
  kratos-db-backup:
  vachan-tbt-ml-models-vol: